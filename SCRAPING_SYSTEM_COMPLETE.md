# ğŸš€ Sistema de Scraping Completo - Scoutea

## ğŸ“‹ Ãndice
1. [VisiÃ³n General](#visiÃ³n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Anti-DDoS y Rate Limiting](#anti-ddos-y-rate-limiting)
4. [Procesamiento en Segundo Plano](#procesamiento-en-segundo-plano)
5. [API Endpoints](#api-endpoints)
6. [ConfiguraciÃ³n y Despliegue](#configuraciÃ³n-y-despliegue)
7. [Monitoreo y Mantenimiento](#monitoreo-y-mantenimiento)

---

## VisiÃ³n General

Sistema de scraping empresarial para actualizar datos de jugadores desde Transfermarkt, diseÃ±ado para procesar **miles de jugadores** sin ser detectado como ataque DDoS.

### âœ¨ CaracterÃ­sticas Principales

- âœ… **Procesamiento AutomÃ¡tico en Segundo Plano** (Vercel Cron)
- âœ… **Anti-DDoS Completo** (User-Agent rotation, random delays, exponential backoff)
- âœ… **Batch Processing** con persistencia de progreso
- âœ… **Rate Limiting Inteligente** con throttling adaptativo
- âœ… **Retry Logic** con tiempos exponenciales
- âœ… **Monitoreo en Tiempo Real** desde el dashboard
- âœ… **Auto-recuperaciÃ³n** de errores temporales
- âœ… **Modo Lento AutomÃ¡tico** cuando detecta problemas

### ğŸ“Š Datos ExtraÃ­dos (16 campos)

Desde la URL de Transfermarkt de cada jugador:

1. `advisor` - Nombre del agente/asesor
2. `url_trfm_advisor` - URL del asesor en Transfermarkt
3. `date_of_birth` - Fecha de nacimiento
4. `team_name` - Equipo actual
5. `team_loan_from` - Equipo de cesiÃ³n (si aplica)
6. `position_player` - PosiciÃ³n en el campo
7. `foot` - Pie dominante (Derecho/Izquierdo/Ambos)
8. `height` - Altura en cm
9. `nationality_1` - Nacionalidad principal
10. `nationality_2` - Segunda nacionalidad (si aplica)
11. `national_tier` - Nivel de selecciÃ³n nacional
12. `agency` - Agencia representante
13. `contract_end` - Fecha fin de contrato
14. `player_trfm_value` - Valor de mercado en â‚¬
15. `photo_coverage` - URL de la foto de perfil del jugador (header)
16. `gallery_photo` - URL de foto de galerÃ­a/cuerpo completo del jugador

---

## Arquitectura del Sistema

### ğŸ—ï¸ Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Admin Panel)                    â”‚
â”‚  /admin/scraping - Dashboard con mÃ©tricas en tiempo real    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API ENDPOINTS                            â”‚
â”‚  â€¢ POST /api/admin/scraping/start   - Iniciar job          â”‚
â”‚  â€¢ POST /api/admin/scraping/process - Procesar batch       â”‚
â”‚  â€¢ GET  /api/admin/scraping/status  - Ver progreso         â”‚
â”‚  â€¢ POST /api/admin/scraping/pause   - Pausar job           â”‚
â”‚  â€¢ POST /api/admin/scraping/cancel  - Cancelar job         â”‚
â”‚  â€¢ GET  /api/admin/scraping/cron    - Cron automÃ¡tico      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VERCEL CRON JOB                           â”‚
â”‚  Schedule: */5 * * * * (cada 5 minutos)                     â”‚
â”‚  Ejecuta: GET /api/admin/scraping/cron                      â”‚
â”‚  Auth: Bearer token (CRON_SECRET)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RATE LIMITER & THROTTLER                       â”‚
â”‚  â€¢ RateLimiter - Retry logic con exponential backoff        â”‚
â”‚  â€¢ AdaptiveThrottler - Ajusta velocidad segÃºn error rate    â”‚
â”‚  â€¢ User-Agent Pool - 20+ agentes rotativos                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFERMARKT                              â”‚
â”‚  Scraping con headers realistas y delays aleatorios         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE (Prisma)                         â”‚
â”‚  â€¢ ScrapingJob - Estado del trabajo                         â”‚
â”‚  â€¢ Jugador - Datos actualizados de jugadores                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“¦ Modelo de Datos (ScrapingJob)

```typescript
model ScrapingJob {
  id              String   @id @default(cuid())
  status          String   // "pending" | "running" | "paused" | "completed" | "failed"

  // Progress tracking
  totalPlayers    Int      @default(0)
  processedCount  Int      @default(0)
  successCount    Int      @default(0)
  errorCount      Int      @default(0)

  // Batch control
  currentBatch    Int      @default(0)
  batchSize       Int      @default(5)      // 5 jugadores por batch

  // Rate limiting metrics
  rateLimitCount  Int      @default(0)      // NÃºmero de 429s recibidos
  retryCount      Int      @default(0)      // Reintentos totales
  errorRate       Float?                    // Porcentaje de errores
  avgDelay        Int?                      // Delay promedio en ms
  slowModeActive  Boolean  @default(false)  // Si estÃ¡ en modo lento
  speedMultiplier Float    @default(1.0)    // 1.0x normal, 2.0x slow, 3.0x very slow
  last429At       DateTime?                 // Ãšltimo rate limit recibido

  // Metadata
  lastError       String?
  startedAt       DateTime?
  completedAt     DateTime?
  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt
  createdBy       String
}
```

---

## Anti-DDoS y Rate Limiting

### ğŸ›¡ï¸ Estrategias Implementadas

#### 1. **User-Agent Rotation** (20+ agentes)

```typescript
// src/lib/scraping/user-agents.ts
const USER_AGENTS = [
  // Windows + Chrome
  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',

  // Mac + Safari
  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15...',

  // Linux + Firefox
  'Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0',

  // Android + Chrome Mobile
  'Mozilla/5.0 (Linux; Android 13; SM-S918B) AppleWebKit/537.36...',

  // ... 20+ total variants
]

export function getRandomUserAgent(): string {
  const randomIndex = Math.floor(Math.random() * USER_AGENTS.length)
  return USER_AGENTS[randomIndex]
}
```

#### 2. **Headers Realistas** (12 headers estÃ¡ndar)

```typescript
export function getRealisticHeaders(referer?: string): Record<string, string> {
  return {
    'User-Agent': getRandomUserAgent(),
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': getRandomAcceptLanguage(),
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Cache-Control': 'max-age=0',
    ...(referer && { 'Referer': referer })
  }
}
```

#### 3. **Random Delays** (5-15 segundos)

```typescript
// Antes: 3 segundos fijos (PREDECIBLE âŒ)
await new Promise(resolve => setTimeout(resolve, 3000))

// Ahora: 5-15 segundos aleatorios (IMPREDECIBLE âœ…)
const randomDelay = Math.floor(Math.random() * (15000 - 5000 + 1)) + 5000
await new Promise(resolve => setTimeout(resolve, randomDelay))
```

#### 4. **Exponential Backoff** (Retry Logic)

```typescript
// src/lib/scraping/rate-limiter.ts
class RateLimiter {
  private calculateBackoffDelay(attempt: number, isRateLimited: boolean): number {
    // Para rate limits (429), usar delays mÃ¡s largos
    const baseDelay = isRateLimited
      ? this.config.baseRetryDelay * 3  // 15 segundos
      : this.config.baseRetryDelay       // 5 segundos

    // Exponential backoff: 2^attempt * baseDelay
    let delay = baseDelay * Math.pow(2, attempt)

    // Jitter aleatorio (Â±20%) para evitar patrones
    const jitter = delay * 0.2 * (Math.random() * 2 - 1)
    delay = delay + jitter

    // Limitar al mÃ¡ximo (2 minutos)
    return Math.min(delay, this.config.maxRetryDelay)
  }
}
```

**Tiempos de retry**:
- Intento 1: 5 segundos (normal) / 15 segundos (429)
- Intento 2: 15 segundos / 45 segundos
- Intento 3: 45 segundos / 120 segundos (mÃ¡ximo)

#### 5. **Adaptive Throttling** (Ajuste automÃ¡tico de velocidad)

```typescript
class AdaptiveThrottler {
  adjustSpeed(errorRate: number) {
    if (errorRate > 50) {
      this.currentMultiplier = 3.0  // Muy lento (15-45s delays)
    } else if (errorRate > 30) {
      this.currentMultiplier = 2.0  // Lento (10-30s delays)
    } else if (errorRate > 15) {
      this.currentMultiplier = 1.5  // Moderado (7-22s delays)
    } else {
      this.currentMultiplier = 1.0  // Normal (5-15s delays)
    }
  }
}
```

#### 6. **Auto-Pause** (PrevenciÃ³n de bloqueos)

```typescript
// Si hay 5 errores 429 consecutivos, pausar automÃ¡ticamente
if (rateLimiter.getConsecutiveRateLimits() >= 5) {
  await prisma.scrapingJob.update({
    where: { id: job.id },
    data: {
      status: 'paused',
      lastError: 'Demasiados rate limits consecutivos. Sistema pausado automÃ¡ticamente.'
    }
  })

  console.error('ğŸ›‘ AUTO-PAUSA: 5 rate limits consecutivos detectados')
}
```

---

## Procesamiento en Segundo Plano

### âš™ï¸ Vercel Cron Job (Plan Hobby)

El sistema usa **Vercel Cron** para procesar scraping automÃ¡ticamente **cada dÃ­a a las 2:00 AM** en el servidor, **sin necesidad de que el usuario tenga la pÃ¡gina abierta**.

#### ConfiguraciÃ³n (`vercel.json`)

```json
{
  "crons": [
    {
      "path": "/api/admin/scraping/cron",
      "schedule": "0 2 * * *"
    }
  ]
}
```

**Schedule syntax** (formato cron estÃ¡ndar):
- `0` - A los 0 minutos
- `2` - A las 2 AM
- `*` - Cada dÃ­a del mes
- `*` - Cada mes
- `*` - Cada dÃ­a de la semana

**âš ï¸ Limitaciones del Plan Hobby**:
- MÃ¡ximo 2 cron jobs por cuenta
- Ejecuciones limitadas a 1 vez al dÃ­a
- Timing puede variar (2:00 AM - 2:59 AM)
- Para ejecuciones mÃ¡s frecuentes, upgrade a Pro ($20/mes)

#### Endpoint Cron (`/api/admin/scraping/cron/route.ts`)

```typescript
export const maxDuration = 300 // 5 minutos mÃ¡ximo

export async function GET(request: Request) {
  // ğŸ” VERIFICAR AUTENTICACIÃ“N (solo Vercel puede llamar)
  const authHeader = request.headers.get('authorization')
  const cronSecret = process.env.CRON_SECRET

  if (process.env.NODE_ENV === 'production') {
    if (!cronSecret || authHeader !== `Bearer ${cronSecret}`) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }
  }

  // ğŸ” BUSCAR JOB ACTIVO
  const job = await prisma.scrapingJob.findFirst({
    where: {
      status: { in: ['pending', 'running'] }
    },
    orderBy: { createdAt: 'desc' }
  })

  if (!job) {
    return NextResponse.json({
      message: 'No hay trabajos activos para procesar'
    })
  }

  // ğŸš€ PROCESAR UN BATCH (idÃ©ntico a /process endpoint)
  // ... lÃ³gica de batch processing con rate limiting

  return NextResponse.json({
    success: true,
    processed: results.length
  })
}
```

#### AutenticaciÃ³n del Cron

**En `.env`**:
```bash
CRON_SECRET=your_secure_random_string_here_233f2d7c2ea67f88ee5449b7890942bacc677f382c0319462d8913314bba3b6b
```

**Vercel automÃ¡ticamente envÃ­a**:
```
Authorization: Bearer <CRON_SECRET>
```

### ğŸ”„ Flujo Completo de Procesamiento (Plan Hobby)

```
1. Admin inicia scraping â†’ POST /api/admin/scraping/start
   â†“
2. Se crea ScrapingJob con status="pending", batchSize=100
   â†“
3. (Usuario cierra la pÃ¡gina - no importa)
   â†“
4. Vercel Cron ejecuta CADA DÃA a las 2 AM â†’ GET /api/admin/scraping/cron
   â†“
5. Encuentra job con status="pending" o "running"
   â†“
6. Procesa 1 batch (100 jugadores) con rate limiting
   â€¢ Delays: 5-15 segundos entre jugadores
   â€¢ DuraciÃ³n total: ~10-20 minutos
   â†“
7. Actualiza progreso en DB (processedCount, successCount, etc.)
   â†“
8. Si hay mÃ¡s jugadores â†’ status sigue "running"
   Si termina â†’ status = "completed"
   Si hay 5 errores 429 consecutivos â†’ status = "paused"
   â†“
9. Espera 24 horas â†’ vuelve al paso 4 (siguiente dÃ­a a las 2 AM)
   â†“
10. (Admin abre la pÃ¡gina en cualquier momento)
    â†’ Fetch GET /api/admin/scraping/status
    â†’ Ve progreso actualizado en tiempo real
```

**â±ï¸ Tiempo Estimado de Completado**:
- 100 jugadores: 1 dÃ­a
- 1,000 jugadores: ~10 dÃ­as
- 10,000 jugadores: ~100 dÃ­as

**ğŸ’¡ Tip**: Si necesitas procesar mÃ¡s rÃ¡pido, puedes ejecutar manualmente batches adicionales desde el dashboard haciendo clic en "Procesar Batch".

---

## API Endpoints

### 1. **POST /api/admin/scraping/start** - Iniciar Scraping

**Request**:
```bash
curl -X POST http://localhost:3000/api/admin/scraping/start \
  -H "Authorization: Bearer <clerk_token>"
```

**Response** (201 Created):
```json
{
  "success": true,
  "message": "Job de scraping creado exitosamente",
  "job": {
    "id": "clxxxxx",
    "status": "pending",
    "totalPlayers": 1523,
    "batchSize": 5
  }
}
```

**Errors**:
- `401` - No autorizado (no autenticado)
- `403` - Acceso denegado (no es admin)
- `409` - Ya existe un job activo

---

### 2. **POST /api/admin/scraping/process** - Procesar Batch

**Request**:
```bash
curl -X POST http://localhost:3000/api/admin/scraping/process \
  -H "Authorization: Bearer <clerk_token>"
```

**Response** (200 OK):
```json
{
  "success": true,
  "message": "Batch procesado: 5 jugadores (5 Ã©xitos, 0 errores)",
  "job": {
    "id": "clxxxxx",
    "status": "running",
    "totalPlayers": 1523,
    "processedCount": 345,
    "successCount": 342,
    "errorCount": 3,
    "currentBatch": 69,
    "progress": 22.7,
    "rateLimitCount": 12,
    "retryCount": 18,
    "errorRate": 0.9,
    "speedMultiplier": 1.0,
    "slowModeActive": false
  }
}
```

**Logic**:
1. Busca jugadores no procesados (limit 5)
2. Para cada jugador:
   - Ejecuta `scrapePlayerData()` con retry logic
   - RotaciÃ³n de User-Agent
   - Headers realistas
   - Timeout 30 segundos
3. Delay aleatorio entre jugadores (5-15s base, ajustado por throttler)
4. Actualiza mÃ©tricas en DB
5. Si detecta 5 errores 429 consecutivos â†’ auto-pause

---

### 3. **GET /api/admin/scraping/status** - Ver Progreso

**Request**:
```bash
curl http://localhost:3000/api/admin/scraping/status \
  -H "Authorization: Bearer <clerk_token>"
```

**Response** (200 OK):
```json
{
  "success": true,
  "job": {
    "id": "clxxxxx",
    "status": "running",
    "totalPlayers": 1523,
    "processedCount": 345,
    "successCount": 342,
    "errorCount": 3,
    "currentBatch": 69,
    "batchSize": 5,
    "rateLimitCount": 12,
    "retryCount": 18,
    "errorRate": 0.9,
    "avgDelay": 8234,
    "slowModeActive": false,
    "speedMultiplier": 1.0,
    "last429At": "2025-10-20T14:32:15.000Z",
    "lastError": null,
    "startedAt": "2025-10-20T10:00:00.000Z",
    "completedAt": null,
    "createdAt": "2025-10-20T10:00:00.000Z",
    "updatedAt": "2025-10-20T14:45:00.000Z",
    "progress": 22.7
  }
}
```

---

### 4. **POST /api/admin/scraping/pause** - Pausar Scraping

**Request**:
```bash
curl -X POST http://localhost:3000/api/admin/scraping/pause \
  -H "Authorization: Bearer <clerk_token>"
```

**Response** (200 OK):
```json
{
  "success": true,
  "message": "Job pausado exitosamente",
  "job": {
    "id": "clxxxxx",
    "status": "paused"
  }
}
```

---

### 5. **POST /api/admin/scraping/cancel** - Cancelar Scraping

**Request**:
```bash
curl -X POST http://localhost:3000/api/admin/scraping/cancel \
  -H "Authorization: Bearer <clerk_token>"
```

**Response** (200 OK):
```json
{
  "success": true,
  "message": "Job cancelado exitosamente",
  "job": {
    "id": "clxxxxx",
    "status": "failed"
  }
}
```

---

### 6. **GET /api/admin/scraping/cron** - Procesamiento AutomÃ¡tico

**Request** (solo Vercel Cron):
```bash
curl http://localhost:3000/api/admin/scraping/cron \
  -H "Authorization: Bearer <CRON_SECRET>"
```

**Response** (200 OK):
```json
{
  "success": true,
  "message": "Batch procesado automÃ¡ticamente",
  "processed": 5,
  "job": {
    "id": "clxxxxx",
    "status": "running",
    "processedCount": 350
  }
}
```

**Response si no hay jobs** (200 OK):
```json
{
  "message": "No hay trabajos activos para procesar"
}
```

---

## ConfiguraciÃ³n y Despliegue

### ğŸ“‹ Requisitos Previos

1. **Database**:
   - PostgreSQL con Prisma configurado
   - Ejecutar migraciÃ³n:
     ```bash
     npx prisma migrate dev --name add_scraping_job_rate_limiting
     ```

2. **Environment Variables** (`.env`):
   ```bash
   # Database
   DATABASE_URL="postgresql://..."
   DIRECT_URL="postgresql://..."

   # Clerk Auth
   NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY="pk_test_..."
   CLERK_SECRET_KEY="sk_test_..."

   # Cron Secret (generar con: openssl rand -hex 32)
   CRON_SECRET="233f2d7c2ea67f88ee5449b7890942bacc677f382c0319462d8913314bba3b6b"
   ```

3. **Vercel Project**:
   - Proyecto configurado en Vercel
   - Variables de entorno configuradas en Vercel Dashboard

### ğŸš€ Despliegue

#### 1. **Deploy a Vercel**

```bash
# Commit y push a GitHub
git add .
git commit -m "feat: Add anti-DDoS scraping system with background processing"
git push origin main

# Vercel auto-deploy desde GitHub
# O deploy manual:
vercel deploy --prod
```

#### 2. **Verificar Cron Job**

DespuÃ©s del deploy:

1. Ve a **Vercel Dashboard** â†’ Tu Proyecto â†’ **Settings** â†’ **Cron Jobs**
2. DeberÃ­as ver:
   ```
   Path: /api/admin/scraping/cron
   Schedule: */5 * * * *
   Status: Active
   ```

3. Ver logs del cron:
   - **Vercel Dashboard** â†’ **Logs** â†’ Filtrar por `/api/admin/scraping/cron`

#### 3. **Configurar Variables de Entorno en Vercel**

En **Settings** â†’ **Environment Variables**, aÃ±adir:

```
CRON_SECRET = 233f2d7c2ea67f88ee5449b7890942bacc677f382c0319462d8913314bba3b6b
DATABASE_URL = postgresql://...
CLERK_SECRET_KEY = sk_test_...
```

### ğŸ§ª Testing Local

Para testear el cron localmente:

```bash
# 1. Iniciar dev server
pnpm dev

# 2. En otra terminal, simular llamada del cron
curl http://localhost:3000/api/admin/scraping/cron \
  -H "Authorization: Bearer 233f2d7c2ea67f88ee5449b7890942bacc677f382c0319462d8913314bba3b6b"
```

---

## Monitoreo y Mantenimiento

### ğŸ“Š Dashboard (/admin/scraping)

El panel muestra en tiempo real:

#### **MÃ©tricas Principales**
- **Total de Jugadores**: Cantidad total a procesar
- **Procesados**: Jugadores ya procesados (exitosos + errores)
- **Exitosos**: Jugadores actualizados correctamente
- **Errores**: Jugadores con fallos en el scraping

#### **MÃ©tricas de Rate Limiting**
- **Rate Limits (429)**: NÃºmero de errores HTTP 429 recibidos
- **Reintentos**: Total de reintentos ejecutados
- **Tasa de Error**: `(errorCount / processedCount) * 100`
- **Velocidad**: Multiplicador actual del throttler
  - `1.0x` = Normal (5-15s)
  - `1.5x` = Moderado (7-22s)
  - `2.0x` = Lento (10-30s)
  - `3.0x` = Muy lento (15-45s)

#### **Alertas**

```typescript
// Alert azul - Scraping activo
{isRunning && (
  <Alert>
    âœ… El sistema procesa automÃ¡ticamente cada 5 minutos
    âœ… Puedes cerrar esta pÃ¡gina - continÃºa en segundo plano
  </Alert>
)}

// Alert amarillo - Modo lento activado
{job.slowModeActive && (
  <Alert variant="warning">
    âš ï¸ Modo lento activado - tasa de error elevada ({errorRate}%)
  </Alert>
)}

// Alert rojo - Pausado por rate limits
{job.status === 'paused' && job.lastError?.includes('rate limit') && (
  <Alert variant="destructive">
    ğŸ›‘ Sistema pausado - demasiados rate limits consecutivos
    Espera 30 minutos y reanuda manualmente
  </Alert>
)}
```

### ğŸ” Logs y Debugging

#### **Logs del Servidor** (desarrollo)

```bash
pnpm dev
```

Buscar logs de:
- `âœ… Job de scraping creado: clxxxxx (1523 jugadores)`
- `ğŸš€ Procesando batch 69 de 304...`
- `âœ… Ã‰xito scraping jugador: John Doe`
- `âš ï¸ Rate limit detectado (3 consecutivos)`
- `ğŸ”„ Reintento 2/3 en 15s...`
- `ğŸ›‘ AUTO-PAUSA: 5 rate limits consecutivos`

#### **Logs de Vercel** (producciÃ³n)

**Vercel Dashboard** â†’ **Logs** â†’ Filtrar:
- `/api/admin/scraping/cron` - Ver ejecuciones del cron
- `/api/admin/scraping/process` - Ver procesamiento manual
- `error` - Ver errores crÃ­ticos

#### **Database Queries** (Prisma Studio)

```bash
npx prisma studio
```

Abrir tabla `ScrapingJob` y verificar:
- `status` - Estado actual del job
- `processedCount` / `totalPlayers` - Progreso
- `rateLimitCount` - CuÃ¡ntos 429s recibidos
- `last429At` - Ãšltimo rate limit (si es reciente, esperar antes de reanudar)
- `errorRate` - Si > 20%, revisar logs para errores

### ğŸ› ï¸ Troubleshooting

#### **Problema: Job se queda en "pending"**

**Causa**: Vercel Cron no se ejecutÃ³ o no encontrÃ³ el job.

**SoluciÃ³n**:
1. Verificar que el cron estÃ¡ activo en Vercel Dashboard
2. Revisar logs del cron: `/api/admin/scraping/cron`
3. Ejecutar manualmente: `POST /api/admin/scraping/process`

---

#### **Problema: Muchos errores 429 (rate limiting)**

**Causa**: Transfermarkt detectÃ³ scraping excesivo.

**SoluciÃ³n**:
1. **Pausar el scraping inmediatamente**
2. **Esperar 30-60 minutos** antes de reanudar
3. **Reducir velocidad**:
   - Editar `/api/admin/scraping/start/route.ts`:
     ```typescript
     batchSize: 3, // Reducir de 5 a 3
     ```
   - Editar `/api/admin/scraping/process/route.ts`:
     ```typescript
     MIN_DELAY_BETWEEN_PLAYERS: 10000, // Aumentar a 10s
     MAX_DELAY_BETWEEN_PLAYERS: 20000, // Aumentar a 20s
     ```
4. **Verificar User-Agents**: Asegurar que hay 20+ agentes en `user-agents.ts`

---

#### **Problema: Scraping muy lento**

**Causa**: Sistema en modo lento por alta tasa de error.

**SoluciÃ³n**:
1. Revisar `errorRate` en el dashboard
2. Si `errorRate > 20%`, investigar errores en logs
3. Errores comunes:
   - `Timeout` â†’ Aumentar `REQUEST_TIMEOUT` a 60000ms
   - `Network error` â†’ Problema de red/DNS
   - `429` â†’ Ver secciÃ³n anterior

---

#### **Problema: Datos no se actualizan**

**Causa**: Scraping extrae HTML pero campos no se guardan.

**SoluciÃ³n**:
1. Verificar que la estructura HTML de Transfermarkt no cambiÃ³
2. Logs del servidor deberÃ­an mostrar:
   ```
   ğŸ” Datos extraÃ­dos: { advisor: 'Jorge Mendes', team_name: 'Real Madrid', ... }
   ```
3. Si no aparece, abrir URL en navegador y comparar estructura HTML
4. Ajustar selectores en `scrapePlayerData()` si es necesario

---

#### **Problema: Cron no ejecuta en producciÃ³n**

**Causa**: ConfiguraciÃ³n incorrecta en Vercel.

**SoluciÃ³n**:
1. Verificar `vercel.json` en root del proyecto:
   ```json
   {
     "crons": [{
       "path": "/api/admin/scraping/cron",
       "schedule": "*/5 * * * *"
     }]
   }
   ```
2. Hacer commit y push a main
3. Redeploy en Vercel
4. Verificar en **Vercel Dashboard** â†’ **Cron Jobs**

---

### ğŸ“ˆ MÃ©tricas de Ã‰xito

**ConfiguraciÃ³n Ã“ptima (Plan Hobby)**:
- **Batch Size**: 100 jugadores/dÃ­a
- **Delay**: 5-15 segundos aleatorios
- **Rate Limit Count**: < 10 por cada 100 jugadores
- **Error Rate**: < 5%
- **Speed Multiplier**: 1.0x (normal)
- **EjecuciÃ³n**: Diaria a las 2:00 AM

**Rendimiento Esperado**:
- **Por ejecuciÃ³n**: ~100 jugadores en 10-20 minutos
- **1,000 jugadores**: ~10 dÃ­as
- **10,000 jugadores**: ~100 dÃ­as

**âš¡ Procesamiento Manual**: Puedes acelerar haciendo clic en "Procesar Batch" manualmente desde el dashboard cuando quieras.

---

## ğŸ¯ PrÃ³ximos Pasos Recomendados

### Mejoras Futuras

1. **Notificaciones**:
   - Enviar email cuando scraping completa
   - Alertas de Slack si hay pausas automÃ¡ticas

2. **MÃ©tricas Avanzadas**:
   - GrÃ¡ficos de progreso en tiempo real (Chart.js)
   - HistÃ³rico de jobs completados

3. **Optimizaciones**:
   - Usar proxies rotativos para evitar rate limits
   - Implementar cache de respuestas HTML

4. **Testing**:
   - Tests unitarios para `RateLimiter` y `AdaptiveThrottler`
   - Tests E2E del flujo completo de scraping

---

## ğŸ“š Referencias

- [Vercel Cron Jobs Documentation](https://vercel.com/docs/cron-jobs)
- [Prisma Client Reference](https://www.prisma.io/docs/orm/prisma-client)
- [Clerk Authentication](https://clerk.com/docs/authentication)
- [Transfermarkt](https://www.transfermarkt.es/)

---

**âœ… Sistema completo y listo para producciÃ³n**

- Anti-DDoS: RotaciÃ³n User-Agent, headers realistas, delays aleatorios
- Background Processing: Vercel Cron cada 5 minutos
- Rate Limiting: Exponential backoff, adaptive throttling, auto-pause
- Monitoreo: Dashboard en tiempo real con mÃ©tricas completas

**Mantenido por**: Equipo Scoutea
**Ãšltima actualizaciÃ³n**: 2025-10-20
